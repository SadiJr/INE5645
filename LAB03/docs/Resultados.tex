\section{\normalsize RESULTADOS}
	Após todo esse detalhamento, ambas as implementações foram submetidas à uma bateria de testes. O ambiente de testes possuí as seguintes configurações:
	
\begin{lstlisting}[frame=single,style=base]
@OS@
Linux s-pc 4.19.85-1-MANJARO #1 SMP PREEMPT Thu Nov 21 10:38:39 UTC 2019 x86_64 GNU/Linux

@CPU@
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   39 bits physical, 48 bits virtual
CPU(s):                          8
On-line CPU(s) list:             0-7
Thread(s) per core:              1
Core(s) per socket:              8
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           158
Model name:                      Intel(R) Core(TM) i7-9700K CPU !!~@~!! 3.60GHz
Stepping:                        12
CPU MHz:                         800.041
CPU max MHz:                     4900,0000
CPU min MHz:                     800,0000
BogoMIPS:                        7202.00
Virtualization:                  VT-x
L1d cache:                       256 KiB
L1i cache:                       256 KiB
L2 cache:                        2 MiB
L3 cache:                        12 MiB
NUMA node0 CPU(s):               0-7

@RAM@
			total			used		free		shared	buff/cache	available
Mem:	32880560	4269840	14884996	220864	13725724	27972388
Swap:	17408220	0				17408220
\end{lstlisting}

	Além disso, o seguinte \textit{script} foi utilizado para automatizar os testes:

\begin{lstlisting}[language=bash,tabsize=2]
#!/bin/bash

if [ ! -f "memory" ]; then
	mpicc -o memory mpi_memory_priority.c
fi

if [ ! -d "process" ]; then
	mpicc -o process mpi_process_priority.c
fi

for h in {1..10}; do
	for i in "memory" "process"; do
		for j in {1..8}; do
			for k in 10 100 1000 10000 100000 1000000 10000000 100000000
			 1000000000 2147483647; do
				mpirun --use-hwthread-cpus -np "$j" "$i" "$k" &>> 
				"$i"_results.txt 
			done
		done
	done
done
\end{lstlisting}

	É importante ressaltar que esse \textit{script}, além de testar a execução das aplicações com diferentes quantidades de processos e tamanhos de vetor, também executa a mesma ação várias vezes, de forma a aumentar a precisão dos resultados. Outro detalhe importante é que o valor 2147483647, passado como último parâmetro na lista de tamanhos de vetores, é exatamente o maior valor que o tipo \textit{int} em C consegue armazenar.
	
	Assim, após várias execuções e uma posterior análise dos resultados, as médias encontradas foram as seguintes (em segundos):

%\begin{sidewaystable}
\begin{flushleft}
{\tiny
\begin{tabular}{|p{1.5cm}|p{1.2cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|p{1.3cm}|}
\hline
Implementação & \multirow{2}{*}{Vetor} & \multicolumn{8}{|c|}{Número de Processos}\\\cline{3-10}\cline{1-1}
\multirow{9}{*}{\shortstack[l]{Paralela com \\gerência de \\memória}} & & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\\cline{2-10}
&10 & 0.000009 & 0.000017 & 0.000023 & 0.000024 & 0.000229 & 0.000052 & 0.000389 & 0.000093\\\cline{2-10}
&100 & 0.000011 & 0.000018 & 0.000040 & 0.000047 & 0.000129 & 0.000042 & 0.000210 & 0.000064\\\cline{2-10}
&1000 & 0.000143 & 0.000158 & 0.000261 & 0.000147 & 0.000155 & 0.000997 & 0.000193 & 0.016705\\\cline{2-10}
&10000 & 0.001109 & 0.001262 & 0.000700 & 0.001206 & 0.002360 & 0.001606 & 0.010954 & 0.006065\\\cline{2-10}
&100000 & 0.013447 & 0.011079 & 0.008159 & 0.012009 & 0.009730 & 0.008960 & 0.004445 & 0.003753\\\cline{2-10}
&1000000 & 0.162323 & 0.087824 & 0.063598 & 0.051636 & 0.044879 & 0.040142 & 0.036031 & 0.035018\\\cline{2-10}
&10000000 & 1.935606 & 0.992100 & 0.695756 & 0.558806 & 0.475215 & 0.426548 & 0.388136 & 0.360975\\\cline{2-10}
&100000000 & 19.883550 & 10.615411 & 7.468864 & 5.906571 & 5.004105 & 4.501016 & 4.088460 & 3.783249\\\cline{2-10}
&1000000000 & 210.191456 & 111.478295 & 76.822245 & 61.574345 & 51.820858 & 46.596178 & 42.480618 & 39.796940\\\cline{2-10}
&2147483647 & 470.984898 & 683.833488 & 308.487683 & 242.086575 & 223.844813 & 214.333666 & 225.258020 & 210.392177\\\hline

\multirow{9}{*}{\shortstack[l]{Paralela sem \\gerência de \\memória}} & 10 &  &  &  &  &  &  &  & \\\cline{2-10}
&100 &  &  &  &  &  &  &  & \\\cline{2-10}
&1000 &  &  &  &  &  &  &  & \\\cline{2-10}
&10000 &  &  &  &  &  &  &  & \\\cline{2-10}
&100000 &  &  &  &  &  &  &  & \\\cline{2-10}
&1000000 &  &  &  &  &  &  &  & \\\cline{2-10}
&10000000 &  &  &  &  &  &  &  & \\\cline{2-10}
&100000000 &  &  &  &  &  &  &  & \\\cline{2-10}
&1000000000 &  &  &  &  &  &  &  & \\\cline{2-10}
&2147483647 &  &  &  &  &  &  &  & \\\hline
\end{tabular}
}
\end{flushleft}
%\end{sidewaystable}

\subsection{\normalsize ALGUMAS CONSIDERAÇÕES}
	\begin{itemize}
		\item A inicialização do MPI é realizada antes de se iniciar a análise do tempo de processamento.
		
		\item A alocação de memória para o vetor original assim como a inserção de elementos aleatórios no mesmo, tanto na versão paralela quanto na sequencial, são realizadas antes de se iniciar a análise do tempo de processamento 

		\item Todos os \textit{outputs} da aplicação estão inseridos em partes do código nas quais ou a análise de tempo de processamento já encerrou ou a mesma ainda não foi iniciada, de forma que tais \textit{outputs} em nada interferem no desempenho final da aplicação.

		\item Os \textit{outputs} que mostravam o vetor antes e depois de ser ordenado, tanto na versão paralela quanto na sequencial, apesar de estarem em partes não críticas do código\footnote{vide item superior} foram comentados para evitar poluição visual\footnote{em casos de vetores com tamanho elevado}. Os mesmos podem ser descomentados à vontade.
	\end{itemize}